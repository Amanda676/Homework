{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparable-membrane",
   "metadata": {},
   "source": [
    "**1. Write simple (straightforward) definitions for the following parameters for RandomForestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClass ifier.html) and indicate how they correlate with the precision and recall for the basic diabetes model we built in class. You will need to rerun the model multiple times to do so.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-amsterdam",
   "metadata": {},
   "source": [
    "  \n",
    "Parameter, Correlation with Precision, Correlation with Recall\n",
    "\n",
    "**estimators:** Number of trees that vote to final result.  Larger is better, but affects computation time.  With fewer tree lower recall and precision, higher number of trees -> higher recall and precision\n",
    "\n",
    "**max_depth:** The depth tree or level of nodes.   More depth is better at precision and recall, but at some point, you may be running an unrestrained tree and either it won't matter or it will overfit.\n",
    "\n",
    "**min_samples_split:** the number of data points left (samples) required to split a node. From 2 - 100.  Increasing slightly from 2, 4, 6, 10, etc increased precision and recall, then for a while precision and recall stayed fairly similar.  At a large enough point (say 50-65) approaching 100, precision and recall decreased as did the model score.  \n",
    "\n",
    "**min_samples_leaf:** samples required to be a leaf node (end node?). The model improved when the min sample went from 1 to 2, but started dropping off by 3 to 4 and a larger drop off by 20.\n",
    "\n",
    "**min_weight_fraction_leaf:** the weighted fraction of the total samples required to be a leaf node.  A fraction of all samples instead of a number.  1% preformed better precision and recall than 5%, 10% or more.   By 40% (ridiculous), drastic negative model performance.\n",
    "\n",
    "**max_leaf_nodes:** best fit for the max number specified.  3 is the sweet spot for this model.  2 wasn't horrible, but more than 4 wasn't helpful.\n",
    "\n",
    "**min_impurity_decrease:** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. (not sure how to rephrase this so I understand it).    Values approaching and equal to zero are better for recall and precision.  .1 broke the model. :)\n",
    "\n",
    "**min_impurity_split:** From the documentation: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  *min_impurity_split has been deprecated in favor of min_impurity_decrease - Use min_impurity_decrease instead.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quarterly-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "diabetes_df = pd.read_csv(\"diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closing-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1)\n",
    "y = diabetes_df['Outcome']\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "#Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "molecular-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state =42, bootstrap=True)\n",
    "#what is an estimator?  models\n",
    "\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equal-sally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82       100\n",
      "           1       0.67      0.59      0.63        54\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.73      0.72      0.72       154\n",
      "weighted avg       0.75      0.75      0.75       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-pencil",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2. How does setting bootstrap=False influence the model performance? Note: the default is bootstrap=True. Explain why your results might be so.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-playing",
   "metadata": {},
   "source": [
    "If False, the whole dataset is used to build each tree.   I think this makes the model perform worse as outliers in the data are used in each tree built.  But I don't see a big difference between the two.\n",
    "\n",
    "Model score fell from 75.3% to 74.7%.\n",
    "\n",
    "There are random differences in each tree, but the same (whole) dataset is used in each tree when bootstrapping is false.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "separated-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467532467532467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, random_state =42, bootstrap=False)\n",
    "#what is an estimator?  models\n",
    "\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fluid-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       100\n",
      "           1       0.65      0.59      0.62        54\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.72      0.71      0.72       154\n",
      "weighted avg       0.74      0.75      0.74       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-birmingham",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
