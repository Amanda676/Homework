{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rapid-therapy",
   "metadata": {},
   "source": [
    "# Week 18 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-orientation",
   "metadata": {},
   "source": [
    "### 1.\tWhat is a neural network? What are the general steps required to build a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-projector",
   "metadata": {},
   "source": [
    "A neural network is a supervised machine deep learning model that uses hidden layers to find interactions between features.  It is supposed to mimic human learning and neurons.  It can also be described a graph model with nodes and edges.\n",
    "\n",
    "To build a neural network, you must create the following layers:\n",
    "    * An input layer\n",
    "    * One of more hidden layers\n",
    "    * An output layer - which may have 1 or more nodes depending on the problem - regression or classification.\n",
    "    \n",
    "For classification problems, each output node represents one category.\n",
    "\n",
    "Each layer must have an activation function (like ReLU) and a number of nodes.   Models are optimized using a function like Adam or Stochastic Gradient Descent.  Neural networks are refined using forward and backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-addition",
   "metadata": {},
   "source": [
    "### 2.\tGenerally, how do you check the performance of a neural network? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-watson",
   "metadata": {},
   "source": [
    "Hold back some of the data to test accuracy/performance using a validation split.  Kfold cross validation is too computationally expensive and takes too long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-supplier",
   "metadata": {},
   "source": [
    "### 3.\tCreate a neural network using keras to predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intermediate-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History \n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "logical-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equivalent-equivalent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "      <th>sex_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  sex_cat  \n",
       "0         0.150     15        2  \n",
       "1         0.070      7        2  \n",
       "2         0.210      9        0  \n",
       "3         0.155     10        2  \n",
       "4         0.055      7        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df = pd.read_csv('clean_abalone_data.csv', index_col=0)\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-weekend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "      <th>sex_cat</th>\n",
       "      <th>sex__F</th>\n",
       "      <th>sex__I</th>\n",
       "      <th>sex__M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  sex_cat  sex__F  sex__I  sex__M  \n",
       "0         0.150     15        2       0       0       1  \n",
       "1         0.070      7        2       0       0       1  \n",
       "2         0.210      9        0       1       0       0  \n",
       "3         0.155     10        2       0       0       1  \n",
       "4         0.055      7        1       0       1       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode sex feature\n",
    "abalone_df = pd.get_dummies(abalone_df, prefix_sep=\"__\",\n",
    "                              columns=['sex'])\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "elegant-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features and target \n",
    "X = abalone_df.drop(['rings', 'sex_cat'], axis=1)\n",
    "y = abalone_df['rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pediatric-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data - important for neural networks, not for trees\n",
    "# did not scale data for last week's tree models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "split-error",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features\n",
    "n_cols = X.shape[1]\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-dream",
   "metadata": {},
   "source": [
    "### Defining helping function to run model\n",
    "\n",
    "After running this a lot of times, I realized the best way to approach this is to a the mean of 10 model runs.   I built helper functions to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acceptable-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, layers, nodes, n_cols):\n",
    "    # function to run a particular model once\n",
    "    # X - features\n",
    "    # y - target\n",
    "    # layers - int number of hidden layers\n",
    "    # nodes - int number of nodes per layer\n",
    "    # n_cols for input shape\n",
    "    # returns MSE of validation data\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(nodes, activation='relu', input_shape=(n_cols,)))\n",
    "    if layers > 1:\n",
    "        for i in range(layers-1):\n",
    "            model.add(Dense(nodes, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='mean_squared_error')\n",
    "    early_stopping_monitor = EarlyStopping(patience=3, monitor='val_loss')\n",
    "    history = History()\n",
    "    model.fit(X, y, validation_split=.3, epochs=50, callbacks=[early_stopping_monitor, history], verbose=1)\n",
    "    # changed verbose=1 from 0 to see keras in action again\n",
    "    \n",
    "    #return last validation loss\n",
    "    return history.history['val_loss'][-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crucial-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_rmse(X, y, layers, nodes, n_cols):\n",
    "    # runs model 10 times\n",
    "    # prints mse list as a visual validation\n",
    "    # returns the root mean squared error of the average MSE\n",
    "    mse_list = []\n",
    "    for i in range(10):\n",
    "        mse = run_model(X, y, layers, nodes, n_cols)\n",
    "        mse_list.append(mse)\n",
    "    print(mse_list)\n",
    "    \n",
    "    mean_mse = mean(mse_list)\n",
    "    rmse = mean_mse**.5\n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-prevention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 55.5198 - val_loss: 6.9019\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.3322 - val_loss: 3.8438\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9213 - val_loss: 3.6871\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9117 - val_loss: 3.6127\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0272 - val_loss: 3.4825\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5782 - val_loss: 3.4230\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6979 - val_loss: 3.4507\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4868 - val_loss: 3.4556\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5767 - val_loss: 3.3538\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2980 - val_loss: 3.3785\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5655 - val_loss: 3.3478\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.3514 - val_loss: 3.4115\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5285 - val_loss: 3.2932\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5436 - val_loss: 3.5938\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3545 - val_loss: 3.3089\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3532 - val_loss: 3.2599\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3246 - val_loss: 3.2643\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3639 - val_loss: 3.3460\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3068 - val_loss: 3.2723\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 57.2476 - val_loss: 6.6470\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.1424 - val_loss: 3.8225\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9433 - val_loss: 3.6582\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9221 - val_loss: 3.5479\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9835 - val_loss: 3.4171\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5501 - val_loss: 3.3765\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6664 - val_loss: 3.4355\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4872 - val_loss: 3.4186\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5563 - val_loss: 3.3240\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2922 - val_loss: 3.3473\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5327 - val_loss: 3.3229\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3559 - val_loss: 3.3781\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5231 - val_loss: 3.2685\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5307 - val_loss: 3.5822\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3610 - val_loss: 3.2835\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3600 - val_loss: 3.2563\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3327 - val_loss: 3.2602\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4062 - val_loss: 3.3259\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3158 - val_loss: 3.2514\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3893 - val_loss: 3.2907\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2579 - val_loss: 3.2172\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1946 - val_loss: 3.2487\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2808 - val_loss: 3.2670\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2188 - val_loss: 3.2133\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.0936 - val_loss: 3.2824\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3266 - val_loss: 3.2034\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3389 - val_loss: 3.2179\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.1776 - val_loss: 3.3224\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3273 - val_loss: 3.2324\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 53.0622 - val_loss: 6.5115\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.1328 - val_loss: 3.8681\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0475 - val_loss: 3.7022\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9929 - val_loss: 3.6133\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0921 - val_loss: 3.4728\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6080 - val_loss: 3.4152\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7326 - val_loss: 3.4674\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5294 - val_loss: 3.4522\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6161 - val_loss: 3.3541\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3457 - val_loss: 3.3780\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5625 - val_loss: 3.3548\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4026 - val_loss: 3.4188\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 1s 4ms/step - loss: 56.5655 - val_loss: 7.5315\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.5452 - val_loss: 3.7702\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.8765 - val_loss: 3.6447\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.8680 - val_loss: 3.5970\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0339 - val_loss: 3.4752\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6051 - val_loss: 3.4207\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6830 - val_loss: 3.4738\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5052 - val_loss: 3.4441\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5830 - val_loss: 3.3689\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3260 - val_loss: 3.3924\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5601 - val_loss: 3.3583\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3698 - val_loss: 3.4204\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5608 - val_loss: 3.3228\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5447 - val_loss: 3.6324\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3884 - val_loss: 3.3520\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3635 - val_loss: 3.3065\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3386 - val_loss: 3.3144\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4267 - val_loss: 3.3601\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.3036 - val_loss: 3.3038\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4114 - val_loss: 3.3391\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2830 - val_loss: 3.2554\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1930 - val_loss: 3.2787\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2752 - val_loss: 3.3167\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2104 - val_loss: 3.2411\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.0826 - val_loss: 3.3236\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3269 - val_loss: 3.2356\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3198 - val_loss: 3.2486\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1922 - val_loss: 3.3219\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3280 - val_loss: 3.2520\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 56.6541 - val_loss: 6.5533\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.1587 - val_loss: 3.8037\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9850 - val_loss: 3.6605\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9357 - val_loss: 3.5720\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.0351 - val_loss: 3.4533\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6073 - val_loss: 3.4105\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6918 - val_loss: 3.4551\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5304 - val_loss: 3.4328\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5944 - val_loss: 3.3470\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3242 - val_loss: 3.3951\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5837 - val_loss: 3.3580\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3954 - val_loss: 3.4214\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 57.3795 - val_loss: 7.1128\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.3279 - val_loss: 3.7947\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9431 - val_loss: 3.7066\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9809 - val_loss: 3.6189\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0699 - val_loss: 3.5129\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6156 - val_loss: 3.4445\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6983 - val_loss: 3.4786\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5445 - val_loss: 3.5154\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5957 - val_loss: 3.3827\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3210 - val_loss: 3.4370\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5990 - val_loss: 3.3840\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3689 - val_loss: 3.4947\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 54.1187 - val_loss: 7.2025\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.4103 - val_loss: 3.8532\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9877 - val_loss: 3.7022\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.9657 - val_loss: 3.5852\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0500 - val_loss: 3.4788\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6134 - val_loss: 3.4111\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6887 - val_loss: 3.4524\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5108 - val_loss: 3.4382\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5960 - val_loss: 3.3332\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3075 - val_loss: 3.3467\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5415 - val_loss: 3.3332\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3689 - val_loss: 3.3917\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5271 - val_loss: 3.2756\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5435 - val_loss: 3.6086\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3515 - val_loss: 3.2889\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3646 - val_loss: 3.2489\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3440 - val_loss: 3.2634\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3981 - val_loss: 3.3393\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3290 - val_loss: 3.2617\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 66.1474 - val_loss: 8.5198\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 6.2075 - val_loss: 3.9632\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0473 - val_loss: 3.7538\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9687 - val_loss: 3.6316\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.1006 - val_loss: 3.5519\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6419 - val_loss: 3.4824\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7411 - val_loss: 3.5187\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5598 - val_loss: 3.4722\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6246 - val_loss: 3.4217\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3657 - val_loss: 3.3804\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5877 - val_loss: 3.3854\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4035 - val_loss: 3.4551\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5998 - val_loss: 3.3248\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5783 - val_loss: 3.6775\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4026 - val_loss: 3.3520\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3923 - val_loss: 3.3178\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3570 - val_loss: 3.3128\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4089 - val_loss: 3.3475\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3370 - val_loss: 3.3016\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4384 - val_loss: 3.3049\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2847 - val_loss: 3.2485\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2170 - val_loss: 3.2612\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2943 - val_loss: 3.3100\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2415 - val_loss: 3.2273\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1053 - val_loss: 3.3021\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3339 - val_loss: 3.2194\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.3379 - val_loss: 3.2233\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2042 - val_loss: 3.3087\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3571 - val_loss: 3.2308\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 54.1518 - val_loss: 7.5402\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.6341 - val_loss: 3.7442\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9250 - val_loss: 3.5726\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9161 - val_loss: 3.4967\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.9992 - val_loss: 3.4034\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5810 - val_loss: 3.3550\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7109 - val_loss: 3.4103\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5014 - val_loss: 3.4220\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5654 - val_loss: 3.3197\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3167 - val_loss: 3.3447\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5523 - val_loss: 3.3290\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.3867 - val_loss: 3.3945\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 59.1336 - val_loss: 6.9655\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 5.4399 - val_loss: 3.8989\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0589 - val_loss: 3.7375\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0758 - val_loss: 3.5597\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.0846 - val_loss: 3.4649\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6253 - val_loss: 3.4195\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7242 - val_loss: 3.4599\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5488 - val_loss: 3.4185\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6074 - val_loss: 3.3528\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3257 - val_loss: 3.3210\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5347 - val_loss: 3.3435\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3861 - val_loss: 3.3846\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5509 - val_loss: 3.2752\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.5415 - val_loss: 3.6144\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3715 - val_loss: 3.2968\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3512 - val_loss: 3.2663\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.3498 - val_loss: 3.2703\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3893 - val_loss: 3.3031\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3246 - val_loss: 3.2490\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3919 - val_loss: 3.2828\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2553 - val_loss: 3.1995\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1508 - val_loss: 3.2298\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2639 - val_loss: 3.2832\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1952 - val_loss: 3.1978\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.0593 - val_loss: 3.3075\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2772 - val_loss: 3.1887\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3204 - val_loss: 3.1953\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1293 - val_loss: 3.3070\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2808 - val_loss: 3.2264\n",
      "[3.272320508956909, 3.232393980026245, 3.4187707901000977, 3.2519590854644775, 3.421375036239624, 3.4947404861450195, 3.2616567611694336, 3.230750799179077, 3.3944783210754395, 3.2263786792755127]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8222190990007716"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "average_model_rmse(X, y, 2, 100, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-scholar",
   "metadata": {},
   "source": [
    "#### Model 1\n",
    "\n",
    "Layers = 1, Nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "acquired-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.284062623977661, 3.273653507232666, 3.3759284019470215, 3.2524707317352295, 3.434234142303467, 3.2776827812194824, 3.314272165298462, 3.4065635204315186, 3.457488775253296, 3.2557079792022705]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.825707113109906"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 1, 100, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-aging",
   "metadata": {},
   "source": [
    "#### Model 2 \n",
    "\n",
    "Layers = 2, Nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "automotive-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.284374237060547, 3.2499232292175293, 3.2461225986480713, 3.268390655517578, 3.2319915294647217, 3.248373031616211, 3.429248094558716, 3.21582293510437, 3.2683210372924805, 3.2360286712646484]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8077222137193776"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 2, 100, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-memorabilia",
   "metadata": {},
   "source": [
    "#### Model 3 \n",
    "\n",
    "Layers = 2, Nodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "following-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4066529273986816, 3.362492322921753, 3.3257672786712646, 3.3973565101623535, 3.360968828201294, 3.339843511581421, 3.4071311950683594, 3.3765745162963867, 3.3637523651123047, 3.3493335247039795]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.835480127381329"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 2, 200, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-poultry",
   "metadata": {},
   "source": [
    "#### Model 4\n",
    "\n",
    "Layers = 3, Nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "romantic-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.297391891479492, 3.3103384971618652, 3.2883212566375732, 3.345993995666504, 3.2697465419769287, 3.3393874168395996, 3.3076107501983643, 3.2888870239257812, 3.3017842769622803, 3.317643642425537]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8184362868485089"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 3, 100, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-specialist",
   "metadata": {},
   "source": [
    "#### Model 5\n",
    "\n",
    "Layers = 4, Nodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "incorrect-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4114432334899902, 3.3468828201293945, 3.3339128494262695, 3.392328977584839, 3.386967420578003, 3.39839506149292, 3.338188409805298, 3.3435137271881104, 3.3187456130981445, 3.3416478633880615]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8333582840290936"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 4, 100, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-bumper",
   "metadata": {},
   "source": [
    "#### Model 6\n",
    "\n",
    "Layers = 3, Nodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "random-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.461167335510254, 3.3578155040740967, 3.427490472793579, 3.4539945125579834, 3.2534005641937256, 3.3252944946289062, 3.2695159912109375, 3.379878520965576, 3.4582760334014893, 3.3128271102905273]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8357467292528966"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 3, 50, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-valentine",
   "metadata": {},
   "source": [
    "#### Model 7\n",
    "\n",
    "Layers = 3, Nodes = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "prime-response",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3090052604675293, 3.266998529434204, 3.3159334659576416, 3.296473503112793, 3.313354730606079, 3.3292856216430664, 3.368403196334839, 3.394998550415039, 3.3207478523254395, 3.307285785675049]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.822703664778553"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 3, 125, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-closure",
   "metadata": {},
   "source": [
    "#### Model 8\n",
    "\n",
    "Layers = 2, Nodes = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "paperback-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.2502245903015137, 3.23888897895813, 3.257948160171509, 3.230085611343384, 3.2475154399871826, 3.243337869644165, 3.2287466526031494, 3.2568018436431885, 3.212043285369873, 3.2422375679016113]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8002174868588436"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_model_rmse(X, y, 2, 75, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-blast",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "| Model Number | Hidden Layers | Nodes | Root Mean Sq Error |\n",
    "|--- | --- | --- | ----|\n",
    "|1|1|100|1.8257|\n",
    "| 2|2 |100 |1.8077|\n",
    "|3|2|200|1.8355|\n",
    "|4| 3|100|1.8184|\n",
    "|5|4|100|1.8334|\n",
    "|6|3|50|1.8357|\n",
    "|7|3|125|1.8227|\n",
    "|8|2|75|1.8002|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-friend",
   "metadata": {},
   "source": [
    "The best performing model had 2 hidden layers and 75 nodes and the 2 layer, 100 node model was also very close.  The average error was 1.8 rings.\n",
    "\n",
    "So based on the features of an abalone, the number of rings (related to age) could be predicted within 1.8 rings. (I think?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-copper",
   "metadata": {},
   "source": [
    "### 4.\tWrite another algorithm to predict the same result as the previous question using either KNN or logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-carnival",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "I wouldn't have chosen this model, but decided to run a simple, untuned version.  Since my models last week and this week treated this problem as linear prediction output, I can't really compare the output and performance.  I tried using RMSE, but that's not usually applicable to a Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "gross-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# The data is already scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "domestic-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplest logistic regression approach\n",
    "modelLR = LogisticRegression(max_iter = 10000, random_state=42)\n",
    "modelLR.fit(X_train, y_train)\n",
    "\n",
    "y_pred= modelLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "expired-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "apart-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1297918678441348\n"
     ]
    }
   ],
   "source": [
    "# this is probably not a valid test\n",
    "\n",
    "RMSE = MSE(y_test, y_pred)**.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "going-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27386934673366836"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "raised-asset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27386934673366836"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-intro",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "RMSE = 1.9542\n",
    "\n",
    "Running this model makes more sense in how I ran all my other abalone data.\n",
    "\n",
    "The RMSE for a simple Linear Regression is worse than the Keras model, but better than some of my Decision Trees from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "marine-mumbai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4900459741284562"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "seven-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9542222317684228\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "RMSE = MSE(y_test, y_pred)**.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-indication",
   "metadata": {},
   "source": [
    "### 5.\tCreate a neural network using pytorch to predict the same result as question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ancient-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "animal-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "aboriginal-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6290, -1.6428, -1.7304,  ..., -0.6701,  1.4290, -0.7523],\n",
      "        [-1.1486, -1.1151, -1.3189,  ..., -0.6701, -0.6998,  1.3293],\n",
      "        [-1.5854, -1.6428, -1.7304,  ..., -0.6701,  1.4290, -0.7523],\n",
      "        ...,\n",
      "        [-1.9785, -1.8538, -1.5933,  ..., -0.6701,  1.4290, -0.7523],\n",
      "        [ 1.1664,  1.1011,  0.3273,  ...,  1.4923, -0.6998, -0.7523],\n",
      "        [ 0.1181, -0.0070,  0.0530,  ..., -0.6701, -0.6998,  1.3293]])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.FloatTensor(y_train.to_numpy())\n",
    "y_test = torch.FloatTensor(y_test.to_numpy())\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "sudden-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=10, hidden1=100, hidden2=100, out_features =1):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "continent-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "answering-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Rprop(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "amended-diploma",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 86.2098388671875\n",
      "Epoch number: 11 with loss: 7.514674663543701\n",
      "Epoch number: 21 with loss: 3.8004727363586426\n",
      "Epoch number: 31 with loss: 3.47163724899292\n",
      "Epoch number: 41 with loss: 3.2960548400878906\n",
      "Epoch number: 51 with loss: 3.1216843128204346\n",
      "Epoch number: 61 with loss: 3.0475783348083496\n",
      "Epoch number: 71 with loss: 2.992058753967285\n",
      "Epoch number: 81 with loss: 2.945040464401245\n",
      "Epoch number: 91 with loss: 2.9107394218444824\n",
      "Epoch number: 101 with loss: 2.883209705352783\n",
      "Epoch number: 111 with loss: 2.854099988937378\n",
      "Epoch number: 121 with loss: 2.83445405960083\n",
      "Epoch number: 131 with loss: 2.8181657791137695\n",
      "Epoch number: 141 with loss: 2.7982242107391357\n",
      "Epoch number: 151 with loss: 2.7869207859039307\n",
      "Epoch number: 161 with loss: 2.7774386405944824\n",
      "Epoch number: 171 with loss: 2.766237497329712\n",
      "Epoch number: 181 with loss: 2.754946708679199\n",
      "Epoch number: 191 with loss: 2.743060350418091\n",
      "Epoch number: 201 with loss: 2.7311606407165527\n",
      "Epoch number: 211 with loss: 2.7198550701141357\n",
      "Epoch number: 221 with loss: 2.708340883255005\n",
      "Epoch number: 231 with loss: 2.6970808506011963\n",
      "Epoch number: 241 with loss: 2.6875131130218506\n",
      "Epoch number: 251 with loss: 2.675337076187134\n",
      "Epoch number: 261 with loss: 2.6641178131103516\n",
      "Epoch number: 271 with loss: 2.6519877910614014\n",
      "Epoch number: 281 with loss: 2.638932466506958\n",
      "Epoch number: 291 with loss: 2.6287031173706055\n",
      "Epoch number: 301 with loss: 2.6162610054016113\n",
      "Epoch number: 311 with loss: 2.604902505874634\n",
      "Epoch number: 321 with loss: 2.595024347305298\n",
      "Epoch number: 331 with loss: 2.586625576019287\n",
      "Epoch number: 341 with loss: 2.576681137084961\n",
      "Epoch number: 351 with loss: 2.565915584564209\n",
      "Epoch number: 361 with loss: 2.5554027557373047\n",
      "Epoch number: 371 with loss: 2.5443828105926514\n",
      "Epoch number: 381 with loss: 2.534573554992676\n",
      "Epoch number: 391 with loss: 2.525818347930908\n",
      "Epoch number: 401 with loss: 2.5180695056915283\n",
      "Epoch number: 411 with loss: 2.5104804039001465\n",
      "Epoch number: 421 with loss: 2.5013434886932373\n",
      "Epoch number: 431 with loss: 2.4923806190490723\n",
      "Epoch number: 441 with loss: 2.4852817058563232\n",
      "Epoch number: 451 with loss: 2.4772250652313232\n",
      "Epoch number: 461 with loss: 2.470028877258301\n",
      "Epoch number: 471 with loss: 2.4637515544891357\n",
      "Epoch number: 481 with loss: 2.4579572677612305\n",
      "Epoch number: 491 with loss: 2.452413558959961\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred[:,0], y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "coordinate-inclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.566014048468276"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.4524**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "therapeutic-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = model(data)\n",
    "        y_pred.append(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "latest-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.169902801513672,\n",
       " 12.285325050354004,\n",
       " 6.628574848175049,\n",
       " 13.747121810913086,\n",
       " 7.7859110832214355,\n",
       " 5.466454982757568,\n",
       " 6.459367275238037,\n",
       " 6.684447765350342,\n",
       " 10.25206184387207,\n",
       " 6.199649810791016,\n",
       " 7.299482822418213,\n",
       " 6.215028762817383,\n",
       " 8.770575523376465,\n",
       " 11.107390403747559,\n",
       " 9.816902160644531,\n",
       " 11.087135314941406,\n",
       " 9.090593338012695,\n",
       " 6.663367748260498,\n",
       " 10.299430847167969]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1:20]  #testing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "annoying-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE  1.9299\n"
     ]
    }
   ],
   "source": [
    "rmse = MSE(y_test, y_pred)**.5\n",
    "print('RMSE ', round(rmse, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-grant",
   "metadata": {},
   "source": [
    "I think there are proabaly parameters that can be tuned to get a pytorch answer more similar to Keras.  Also note that Pytorch model is overfitted.  I found this to be less of a problem with the Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-marks",
   "metadata": {},
   "source": [
    "### 6.\tCompare the performance of the neural networks to the other model you created. Which performed better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-messenger",
   "metadata": {},
   "source": [
    "The **Keras neural network out performed every model** for the the Abalone data set so far.  In part, I was better able to tune the Keras model, so it's not surprising that it out performed the Pytorch model.  If I had more time, I would probably write functions and tune the Pytorch model like I did the Keras model.   Have a simple function for Keras calls made it much simpler to experiment with different layers and nodes.\n",
    "\n",
    "In general, **neural networks** have the opportunity to outperform other models due to the fact they can **better account for how different features interact with each other.**\n",
    "\n",
    "***Best Keras Test set RMSE: 1.8002***\n",
    "\n",
    "***Random Forest Test set RMSE: 1.886 (last week)***\n",
    "\n",
    "***Linear Regression RMSE: 1.9542***\n",
    "\n",
    "What was interesting was how well the 'simple' Linear Regression model worked on the scaled data.  I feel this is the could be the worse case scenario - where **all other models should improve on a Linear Regression score.**\n",
    "\n",
    "It might have been interesting to treat the Abalone data as a classification problem, but I think the target would need to be binned in 2-4 bins for a meaniful model.\n",
    "\n",
    "I chose **not to treat the Abalone data as a classification problem** because I thought it would be more meaniful to have a model predict an age (with rings) with a margin of error.   So if you're the person in the lab wanting **to know approximate age** of the abalone specimen,**all you need is an easier to gather feature set.**   The information about the data mentioned it was tedious and error prone to count rings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-recall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blocked-sucking",
   "metadata": {},
   "source": [
    "Some things for fun - I'm running my best Keras model to check model MSE and test MSE.   I changed the my function to verbose = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scheduled-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 61.7383 - val_loss: 9.0737\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 6.7556 - val_loss: 4.1968\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 4.3241 - val_loss: 3.8679\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.1032 - val_loss: 3.7066\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 4.2044 - val_loss: 3.6027\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7229 - val_loss: 3.5150\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.7752 - val_loss: 3.5297\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6214 - val_loss: 3.4931\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6380 - val_loss: 3.4670\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3759 - val_loss: 3.4165\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6043 - val_loss: 3.4393\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4381 - val_loss: 3.4621\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6219 - val_loss: 3.3759\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.6086 - val_loss: 3.7123\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4375 - val_loss: 3.3655\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4166 - val_loss: 3.3140\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3812 - val_loss: 3.3612\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4161 - val_loss: 3.3471\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3664 - val_loss: 3.3012\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.4594 - val_loss: 3.2979\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3210 - val_loss: 3.2845\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2389 - val_loss: 3.2859\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3435 - val_loss: 3.2765\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2669 - val_loss: 3.2661\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.1309 - val_loss: 3.3087\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3595 - val_loss: 3.2412\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3587 - val_loss: 3.2660\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.2183 - val_loss: 3.3283\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 3.3619 - val_loss: 3.2498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.249844551086426"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(X, y, 2, 75, n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proud-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE:  1.8354835875049387\n",
      "Test RMSE:  1.802720166858961\n"
     ]
    }
   ],
   "source": [
    "modelRMSE = 3.369**.5\n",
    "testRMSE = 3.2498**.5\n",
    "\n",
    "print('Model RMSE: ', modelRMSE)\n",
    "print('Test RMSE: ', testRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-recognition",
   "metadata": {},
   "source": [
    "Model and test RMSE for Keras model are pretty close, so I don't feel it's over or under fitted unlike the pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-plastic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
